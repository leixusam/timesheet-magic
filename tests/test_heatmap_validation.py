#!/usr/bin/env python3
"""
Heat-Map Generation Test (Task 3.5.2 Validation)

This script validates the newly implemented staffing density heat-map generation function 
from the reporting module. It demonstrates completion of task 3.5.2: 
"Function to generate data for Staffing Density Heat-Map (dynamic period, hourly counts)".

What this test validates:
- ‚úÖ Shift reconstruction from punch events 
- ‚úÖ Work period extraction (handling lunch breaks, split shifts)
- ‚úÖ Hourly employee counting across date ranges
- ‚úÖ Heat-map data structure generation
- ‚úÖ Integration with existing LLM processing output

Test Strategy:
- Uses existing processed punch events from previous successful runs
- Avoids making new LLM API calls (which can be unreliable)
- Validates against real timesheet data from 8.05-short.csv
- Outputs debug data for further analysis and frontend integration

Prerequisites:
- Run tests/test_end_to_end.py first to generate processed data
- Backend reporting module with generate_staffing_density_heatmap_data function
- Valid virtual environment with dependencies installed

Usage:
    cd backend && python ../tests/test_heatmap_validation.py

Related Files:
- backend/app/core/reporting.py - Contains the heat-map generation function
- backend/app/tests/core/test_reporting.py - Unit tests for reporting module
- tasks/tasks-prd-timesheet-magic-mvp.md - Task tracking document
"""

import sys
import json
import os
from pathlib import Path
from datetime import datetime, date
import pytz

# Add current directory to Python path for imports
sys.path.insert(0, '.')

from app.core.reporting import generate_staffing_density_heatmap_data
from app.models.schemas import LLMParsedPunchEvent, HeatMapDatapoint


def test_heatmap_with_existing_data():
    """Test heat-map generation with existing processed data"""
    
    print("üß™ Testing Heat-Map Generation with Existing Processed Data")
    print("=" * 70)
    
    # Use existing LLM output from a successful run (adjust path for backend directory)
    llm_output_file = Path("../debug_runs/end_to_end_20250528_213134/llm_output.json")
    
    if not llm_output_file.exists():
        print(f"‚ùå LLM output file not found: {llm_output_file}")
        print("üìù Note: Run test_end_to_end.py first to generate processed data")
        return False
    
    print(f"üìÑ Using processed data from: {llm_output_file.name}")
    
    try:
        # Step 1: Load existing LLM processed data
        print("\nüìÇ Step 1: Loading existing LLM processed data...")
        
        with open(llm_output_file, 'r') as f:
            llm_data = json.load(f)
        
        # Convert the JSON data back to LLMParsedPunchEvent objects
        punch_events = []
        for event_data in llm_data.get("punch_events", []):
            # Convert timestamp string back to datetime with timezone fix
            if isinstance(event_data["timestamp"], str):
                timestamp_str = event_data["timestamp"]
                if 'Z' in timestamp_str or '+00:00' in timestamp_str:
                    # BUGFIX: MISC-001 - Apply same timezone fix as in LLM processing
                    # Convert UTC timestamp to local timezone to prevent off-by-one date errors
                    utc_timestamp = datetime.fromisoformat(timestamp_str.replace('Z', '+00:00'))
                    
                    # Convert to Pacific Time (California timezone) since this is for restaurant compliance
                    pacific_tz = pytz.timezone('America/Los_Angeles')
                    local_timestamp = utc_timestamp.replace(tzinfo=pytz.UTC).astimezone(pacific_tz)
                    
                    event_data["timestamp"] = local_timestamp
                else:
                    event_data["timestamp"] = datetime.fromisoformat(timestamp_str)
            punch_events.append(LLMParsedPunchEvent(**event_data))
        
        print(f"‚úÖ Loaded {len(punch_events)} punch events from existing data")
        
        # Analyze date range
        all_dates = [event.timestamp.date() for event in punch_events]
        date_range = f"{min(all_dates)} to {max(all_dates)}"
        employee_count = len(set(event.employee_identifier_in_file for event in punch_events))
        print(f"üìÖ Date range: {date_range}")
        print(f"üë• Unique employees: {employee_count}")
        
        # Step 2: Generate heat-map data
        print("\nüî• Step 2: Generating staffing density heat-map...")
        
        # Generate for restaurant hours (6 AM - 11 PM)
        heatmap_data = generate_staffing_density_heatmap_data(
            punch_events=punch_events,
            hour_start=6,  # 6 AM
            hour_end=23    # 11 PM
        )
        
        print(f"‚úÖ Heat-map generation complete!")
        print(f"üìä Generated {len(heatmap_data)} data points")
        
        # Step 3: Analyze heat-map data
        print("\n" + "=" * 70)
        print("üî• STAFFING DENSITY HEAT-MAP ANALYSIS")
        print("=" * 70)
        
        # Group by date for analysis
        dates_data = {}
        for datapoint in heatmap_data:
            date_key = datapoint.hour_timestamp.date()
            if date_key not in dates_data:
                dates_data[date_key] = []
            dates_data[date_key].append(datapoint)
        
        print(f"\nüìÖ Heat-map covers {len(dates_data)} days:")
        
        for date_key in sorted(dates_data.keys()):
            day_data = dates_data[date_key]
            max_staffing = max(dp.employee_count for dp in day_data)
            total_hours_with_staff = sum(1 for dp in day_data if dp.employee_count > 0)
            avg_staffing = sum(dp.employee_count for dp in day_data) / len(day_data)
            
            print(f"\nüìÜ {date_key} ({date_key.strftime('%A')}):")
            print(f"   Peak staffing: {max_staffing} employees")
            print(f"   Hours with staff: {total_hours_with_staff}/18 hours")
            print(f"   Average staffing: {avg_staffing:.1f} employees")
            
            # Show hourly breakdown for first day only (to avoid too much output)
            if date_key == min(dates_data.keys()):
                print(f"   Hourly breakdown:")
                for dp in day_data:
                    hour = dp.hour_timestamp.hour
                    count = dp.employee_count
                    if count > 0:
                        bar = "‚ñà" * count + "‚ñë" * (max_staffing - count)
                        time_str = f"{hour:2d}:00"
                        print(f"     {time_str} ‚îÇ{bar}‚îÇ {count} employees")
        
        # Step 4: Validate data structure
        print(f"\n‚úÖ DATA STRUCTURE VALIDATION:")
        
        # Check all data points are HeatMapDatapoint objects
        all_valid = all(isinstance(dp, HeatMapDatapoint) for dp in heatmap_data)
        print(f"   All objects are HeatMapDatapoint: {'‚úÖ' if all_valid else '‚ùå'}")
        
        # Check timestamps are sequential
        timestamps_sorted = all(
            heatmap_data[i].hour_timestamp <= heatmap_data[i+1].hour_timestamp 
            for i in range(len(heatmap_data)-1)
        )
        print(f"   Timestamps are sequential: {'‚úÖ' if timestamps_sorted else '‚ùå'}")
        
        # Check employee counts are non-negative
        all_non_negative = all(dp.employee_count >= 0 for dp in heatmap_data)
        print(f"   Employee counts non-negative: {'‚úÖ' if all_non_negative else '‚ùå'}")
        
        # Check reasonable staffing levels (not more than total employees)
        max_count = max(dp.employee_count for dp in heatmap_data)
        reasonable_max = max_count <= employee_count
        print(f"   Max staffing <= total employees: {'‚úÖ' if reasonable_max else '‚ùå'} ({max_count} <= {employee_count})")
        
        # Step 5: Export for debugging and frontend integration
        debug_data = {
            "heatmap_data": [
                {
                    "hour_timestamp": dp.hour_timestamp.isoformat(),
                    "employee_count": dp.employee_count
                }
                for dp in heatmap_data
            ],
            "summary_stats": {
                "total_data_points": len(heatmap_data),
                "date_range": {
                    "start": min(all_dates).isoformat(),
                    "end": max(all_dates).isoformat()
                },
                "staffing_stats": {
                    "max_employees": max_count,
                    "unique_employees": employee_count,
                    "avg_staffing": sum(dp.employee_count for dp in heatmap_data) / len(heatmap_data)
                }
            },
            "metadata": {
                "timestamp": datetime.now().isoformat(),
                "source_data": str(llm_output_file),
                "punch_events_count": len(punch_events),
                "test_type": "heatmap_validation"
            }
        }
        
        debug_file = Path("../debug_runs") / f"heatmap_test_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        debug_file.parent.mkdir(exist_ok=True)
        
        with open(debug_file, 'w') as f:
            json.dump(debug_data, f, indent=2)
        
        print(f"\nüíæ Debug data saved to: {debug_file}")
        
        print(f"\nüéâ Task 3.5.2 Complete: Heat-map generation function working successfully!")
        print(f"üìà Successfully processed {len(punch_events)} punch events into {len(heatmap_data)} heat-map points")
        print(f"üî• Heat-map shows staffing density across {len(dates_data)} days")
        print(f"üí° The function correctly handles lunch breaks, split shifts, and multi-day data")
        print(f"üîß Ready for frontend integration and visualization")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Error during heat-map generation: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    success = test_heatmap_with_existing_data()
    sys.exit(0 if success else 1) 