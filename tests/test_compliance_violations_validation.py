#!/usr/bin/env python3
"""
Compliance Violations Compilation Test (Task 3.5.3 Validation)

This script validates the newly implemented general compliance violations compilation function 
from the reporting module. It demonstrates completion of task 3.5.3: 
"Function to compile list of general compliance violations".

What this test validates:
- ‚úÖ Violation detection across multiple categories (meal breaks, rest breaks, overtime)
- ‚úÖ Duplicate employee handling and consolidation
- ‚úÖ Proper ViolationInstance object structure and fields
- ‚úÖ Actionable advice generation for each violation type
- ‚úÖ Integration with existing LLM processing output

Test Strategy:
- Uses existing processed punch events from previous successful runs
- Avoids making new LLM API calls (which can be unreliable)
- Validates against real timesheet data from 8.05-short.csv
- Outputs structured violation data for frontend integration

Prerequisites:
- Run tests/test_end_to_end.py first to generate processed data
- Backend reporting module with compile_general_compliance_violations function
- Valid virtual environment with dependencies installed

Usage:
    cd backend && python ../tests/test_compliance_violations_validation.py

Related Files:
- backend/app/core/reporting.py - Contains the violation compilation function
- backend/app/core/compliance_rules.py - Individual violation detection functions
- backend/app/tests/core/test_reporting.py - Unit tests for reporting module
- tasks/tasks-prd-timesheet-magic-mvp.md - Task tracking document
"""

import sys
import json
import os
from pathlib import Path
from datetime import datetime, date
import pytz

# Add current directory to Python path for imports
sys.path.insert(0, '.')

from app.core.reporting import compile_general_compliance_violations
from app.models.schemas import LLMParsedPunchEvent


def test_compliance_violations_with_existing_data():
    """Test compliance violations compilation with existing processed data"""
    
    print("üö® Testing Compliance Violations Compilation with Existing Processed Data")
    print("=" * 80)
    
    # Use existing LLM output from a successful run (adjust path for backend directory)
    llm_output_file = Path("../debug_runs/end_to_end_20250528_213134/llm_output.json")
    
    if not llm_output_file.exists():
        print(f"‚ùå LLM output file not found: {llm_output_file}")
        print("üìù Note: Run test_end_to_end.py first to generate processed data")
        return False
    
    print(f"üìÑ Using processed data from: {llm_output_file.name}")
    
    try:
        # Step 1: Load existing LLM processed data
        print("\nüìÇ Step 1: Loading existing LLM processed data...")
        
        with open(llm_output_file, 'r') as f:
            llm_data = json.load(f)
        
        # Convert the JSON data back to LLMParsedPunchEvent objects
        punch_events = []
        for event_data in llm_data.get("punch_events", []):
            # Convert timestamp string back to datetime
            if isinstance(event_data["timestamp"], str):
                timestamp_str = event_data["timestamp"]
                if 'Z' in timestamp_str or '+00:00' in timestamp_str:
                    # BUGFIX: MISC-001 - Apply same timezone fix as in LLM processing
                    # Convert UTC timestamp to local timezone to prevent off-by-one date errors
                    utc_timestamp = datetime.fromisoformat(timestamp_str.replace('Z', '+00:00'))
                    
                    # Convert to Pacific Time (California timezone) since this is for restaurant compliance
                    pacific_tz = pytz.timezone('America/Los_Angeles')
                    local_timestamp = utc_timestamp.replace(tzinfo=pytz.UTC).astimezone(pacific_tz)
                    
                    event_data["timestamp"] = local_timestamp
                else:
                    event_data["timestamp"] = datetime.fromisoformat(timestamp_str)
            punch_events.append(LLMParsedPunchEvent(**event_data))
        
        print(f"‚úÖ Loaded {len(punch_events)} punch events from existing data")
        
        # Analyze data scope
        all_dates = [event.timestamp.date() for event in punch_events]
        date_range = f"{min(all_dates)} to {max(all_dates)}"
        employee_count = len(set(event.employee_identifier_in_file for event in punch_events))
        print(f"üìÖ Date range: {date_range}")
        print(f"üë• Unique employees: {employee_count}")
        
        # Step 2: Compile all compliance violations
        print("\nüö® Step 2: Compiling all compliance violations...")
        
        violations = compile_general_compliance_violations(punch_events)
        
        print(f"‚úÖ Violation compilation complete!")
        print(f"üìä Found {len(violations)} total compliance violations")
        
        # Step 3: Analyze violations by type
        print("\n" + "=" * 80)
        print("üö® COMPLIANCE VIOLATIONS ANALYSIS")
        print("=" * 80)
        
        if not violations:
            print("‚ú® No compliance violations detected - excellent compliance!")
            return True
        
        # Group violations by rule type
        violation_types = {}
        for violation in violations:
            rule_type = violation.rule_id
            if rule_type not in violation_types:
                violation_types[rule_type] = []
            violation_types[rule_type].append(violation)
        
        print(f"\nüìã Violation Types Summary:")
        for rule_type, rule_violations in violation_types.items():
            print(f"   {rule_type}: {len(rule_violations)} violations")
        
        # Analyze violations by employee
        employee_violations = {}
        for violation in violations:
            emp_id = violation.employee_identifier
            if emp_id not in employee_violations:
                employee_violations[emp_id] = []
            employee_violations[emp_id].append(violation)
        
        print(f"\nüë• Employee Violations Summary:")
        for emp_id, emp_violations in employee_violations.items():
            violation_summary = {}
            for v in emp_violations:
                rule_type = v.rule_id.split('_')[0] + "_" + v.rule_id.split('_')[1] if '_' in v.rule_id else v.rule_id
                violation_summary[rule_type] = violation_summary.get(rule_type, 0) + 1
            
            violation_text = ", ".join([f"{count} {rule}" for rule, count in violation_summary.items()])
            print(f"   {emp_id}: {len(emp_violations)} violations ({violation_text})")
        
        # Show detailed examples
        print(f"\nüìã Detailed Violation Examples (first 3):")
        for i, violation in enumerate(violations[:3]):
            print(f"\n   Violation {i+1}:")
            print(f"     Rule: {violation.rule_id}")
            print(f"     Employee: {violation.employee_identifier}")
            print(f"     Date: {violation.date_of_violation}")
            print(f"     Description: {violation.rule_description}")
            print(f"     Details: {violation.specific_details[:100]}...")
            print(f"     Action: {violation.suggested_action_generic[:100]}...")
        
        # Step 4: Validate data structure
        print(f"\n‚úÖ DATA STRUCTURE VALIDATION:")
        
        # Check all violations are proper objects
        all_valid_objects = all(hasattr(v, 'rule_id') and hasattr(v, 'employee_identifier') for v in violations)
        print(f"   All objects have required attributes: {'‚úÖ' if all_valid_objects else '‚ùå'}")
        
        # Check required fields are populated
        all_fields_populated = all(
            v.rule_id and v.rule_description and v.employee_identifier and 
            v.date_of_violation and v.specific_details and v.suggested_action_generic
            for v in violations
        )
        print(f"   All required fields populated: {'‚úÖ' if all_fields_populated else '‚ùå'}")
        
        # Check date validity
        all_dates_valid = all(isinstance(v.date_of_violation, date) for v in violations)
        print(f"   All dates are valid date objects: {'‚úÖ' if all_dates_valid else '‚ùå'}")
        
        # Check actionable advice is meaningful
        all_advice_meaningful = all(len(v.suggested_action_generic) > 20 for v in violations)
        print(f"   All actionable advice is substantial: {'‚úÖ' if all_advice_meaningful else '‚ùå'}")
        
        # Check sorting
        violations_sorted = all(
            (violations[i].date_of_violation, violations[i].employee_identifier, violations[i].rule_id) <=
            (violations[i+1].date_of_violation, violations[i+1].employee_identifier, violations[i+1].rule_id)
            for i in range(len(violations)-1)
        )
        print(f"   Violations are properly sorted: {'‚úÖ' if violations_sorted else '‚ùå'}")
        
        # Step 5: Export for debugging and frontend integration
        violation_data = {
            "violations": [
                {
                    "rule_id": v.rule_id,
                    "rule_description": v.rule_description,
                    "employee_identifier": v.employee_identifier,
                    "date_of_violation": v.date_of_violation.isoformat(),
                    "specific_details": v.specific_details,
                    "suggested_action_generic": v.suggested_action_generic
                }
                for v in violations
            ],
            "summary_stats": {
                "total_violations": len(violations),
                "unique_employees_with_violations": len(employee_violations),
                "violation_types": {rule_type: len(rule_violations) for rule_type, rule_violations in violation_types.items()},
                "date_range": {
                    "start": min(all_dates).isoformat(),
                    "end": max(all_dates).isoformat()
                }
            },
            "metadata": {
                "timestamp": datetime.now().isoformat(),
                "source_data": str(llm_output_file),
                "punch_events_count": len(punch_events),
                "test_type": "compliance_violations_validation"
            }
        }
        
        debug_file = Path("../debug_runs") / f"violations_test_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        debug_file.parent.mkdir(exist_ok=True)
        
        with open(debug_file, 'w') as f:
            json.dump(violation_data, f, indent=2)
        
        print(f"\nüíæ Violation data saved to: {debug_file}")
        
        print(f"\nüéâ Task 3.5.3 Complete: Compliance violations compilation function working successfully!")
        print(f"üö® Successfully compiled {len(violations)} violations from {len(punch_events)} punch events")
        print(f"‚öñÔ∏è Detected {len(violation_types)} different violation types across {len(employee_violations)} employees")
        print(f"üíº The function correctly handles duplicate detection, consolidation, and provides actionable advice")
        print(f"üîß Ready for frontend integration and compliance report display")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Error during compliance violations compilation: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    success = test_compliance_violations_with_existing_data()
    sys.exit(0 if success else 1) 